{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from docling.datamodel.base_models import PipelineOptions\n",
    "from docling.datamodel.document import DocumentConversionInput\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.types.doc.base import (\n",
    "    BaseText,\n",
    "    Ref,\n",
    "    Table,\n",
    ")\n",
    "\n",
    "def format_raw_pdf(input_doc_paths,output_dir):\n",
    "    doc_input = DocumentConversionInput.from_paths(input_doc_paths)\n",
    "\n",
    "    pipeline_options = PipelineOptions()\n",
    "    pipeline_options.do_ocr = True\n",
    "    pipeline_options.do_table_structure = True\n",
    "    pipeline_options.table_structure_options.do_cell_matching = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        pipeline_options=pipeline_options\n",
    "    )\n",
    "\n",
    "    result = doc_converter.convert(doc_input)\n",
    "\n",
    "    db_folder_path = output_dir / \"table\" \n",
    "    db_file_path = db_folder_path / \"all_tables.db\"\n",
    "    db_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "    table_id = 0\n",
    "    print(f'Extraction Starting ...')\n",
    "    for r in result:\n",
    "        \n",
    "        output = r.output\n",
    "\n",
    "        text_folder_path = output_dir / \"text\" \n",
    "        text_file_path = text_folder_path / f\"{r.input.file.stem}.txt\"\n",
    "        text_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        text_file_path.open(\"w\").close()\n",
    "\n",
    "        main_text_labels = [\n",
    "            \"paragraph\",\n",
    "            \"table\",\n",
    "        ]\n",
    "\n",
    "        has_title = False\n",
    "        prev_text = \"\"\n",
    "        if output.main_text is not None:\n",
    "            for index,orig_item in enumerate(output.main_text):\n",
    "                item = (\n",
    "                    output._resolve_ref(orig_item)\n",
    "                    if isinstance(orig_item, Ref)\n",
    "                    else orig_item\n",
    "                )\n",
    "                if item is None:\n",
    "                    continue\n",
    "\n",
    "                item_type = item.obj_type\n",
    "                if isinstance(item, BaseText) and item_type in main_text_labels:\n",
    "                    text = item.text\n",
    "\n",
    "                    # ignore repeated text\n",
    "                    if prev_text == text:\n",
    "                        continue\n",
    "                    else:\n",
    "                        prev_text = text\n",
    "\n",
    "                    text_file_path.open(\"a\").write(f\"{text}\\n\\n\")\n",
    "\n",
    "                elif (\n",
    "                    isinstance(item, Table)\n",
    "                    and item.data\n",
    "                    and item_type in main_text_labels\n",
    "                ):\n",
    "                    table = []\n",
    "                    if(item.text == \"Table of Contents\"):\n",
    "                        continue\n",
    "                    for row in item.data:\n",
    "                        tmp = []\n",
    "                        for col in row:\n",
    "                            tmp.append(col.text)\n",
    "                        table.append(tmp)\n",
    "\n",
    "                    if len(table) > 1 and len(table[0]) > 0:\n",
    "                        table_df = pd.DataFrame(table[1:], columns=table[0])\n",
    "\n",
    "                        c = conn.cursor()\n",
    "                        table_name = \"\"\n",
    "                        if(item.text!=\"\"):\n",
    "                            table_name = item.text\n",
    "                        else:\n",
    "                            table_name = f\"table_{chr(ord('a')+table_id)}\"\n",
    "                            table_id+=1\n",
    "                        table_df.to_sql(name=table_name, con=conn, if_exists='replace', index=False)\n",
    "                        conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    print(f'Text and Tables are seperated from the PDF files and stored in the folder {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_doc_paths = [\n",
    "    Path(\"./data/duplicate-chars.pdf\")\n",
    "]\n",
    "\n",
    "output_dir = Path(\"./data/formatted/\")\n",
    "\n",
    "format_raw_pdf(input_doc_paths,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Chain\n",
    "\n",
    "Let's create a simple chain that takes a question, turns it into a SQL query, executes the query, and uses the result to answer the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///./data/formatted/table/all_tables.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "sql_gen_prompt = PromptTemplate(input_variables=['input', 'table_info'], partial_variables={'top_k': '5'}, template= \n",
    "\"\"\"You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\n",
    "SQLQuery: \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = create_sql_query_chain(llm, db)\n",
    "execute_query = QuerySQLDataBaseTool(db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "answer = answer_prompt | llm | StrOutputParser()\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Lower explosion limit of Hydrogen gas is 4.0 vol%.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = sql_chain.invoke({\"question\": \"What is the Lower explosion limit of Hydrogen gas?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Chain\n",
    "\n",
    "We will create a RAG chain with text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "def get_text_documents(text_dir_path):\n",
    "    if os.path.isdir(text_dir_path):\n",
    "        text_documents = []\n",
    "        for file in os.listdir(text_dir_path):\n",
    "            loader = TextLoader(text_dir_path+file)\n",
    "            text_documents.extend(loader.load())\n",
    "    else:\n",
    "        loader = TextLoader(text_dir_path)\n",
    "        text_documents = loader.load()\n",
    "    \n",
    "    return text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/kirushikesh/.conda/multimodalproject/lib/python3.10/site-packages/langsmith/client.py:5301: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs = get_text_documents('data/formatted/text/')\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Riken Keiki GP-1000 is a compact and lightweight gas detector with high sensitivity for hydrocarbons, using a catalytic sensor for measurement. It has a built-in pump with a pump booster function and allows direct selection from a list of 25 hydrocarbons for precise gas alignment. Calibration is only necessary for CH$_{4}$.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Riken Keiki GP-1000?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Modal Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    sql_chain.as_tool(\n",
    "        name=\"sql_database\",\n",
    "        description=\"useful when you need to answer the user question from the tabular data. Use this for selective type questions.\",\n",
    "    ),\n",
    "    rag_chain.as_tool(\n",
    "        name=\"text_database\",\n",
    "        description=\"useful when you need to answer from text data corpus. It will retrieve the text and answer based on it. Use this for descriptive type questions.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `text_database` with `Riken Keiki GP-1000`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe Riken Keiki GP-1000 is a compact gas detector with high sensitivity for hydrocarbons, using a catalytic sensor for measurement. It has a built-in pump with a pump booster function and allows direct selection from a list of 25 hydrocarbons for precise gas alignment. Calibration is only necessary for CH$_{4}$.\u001b[0m\u001b[32;1m\u001b[1;3mThe Riken Keiki GP-1000 is a compact gas detector with high sensitivity for hydrocarbons. It uses a catalytic sensor for measurement and has a built-in pump with a pump booster function. It allows direct selection from a list of 25 hydrocarbons for precise gas alignment, and calibration is only necessary for CH$_{4}$.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Riken Keiki GP-1000?',\n",
       " 'output': 'The Riken Keiki GP-1000 is a compact gas detector with high sensitivity for hydrocarbons. It uses a catalytic sensor for measurement and has a built-in pump with a pump booster function. It allows direct selection from a list of 25 hydrocarbons for precise gas alignment, and calibration is only necessary for CH$_{4}$.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is Riken Keiki GP-1000?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_database` with `{'question': 'What is the Lower explosion limit of Hydrogen gas?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe Lower explosion limit of Hydrogen gas is 4.0 vol%.\u001b[0m\u001b[32;1m\u001b[1;3mThe Lower explosion limit of Hydrogen gas is 4.0 vol%.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Identify the Lower explosion limit of Hydrogen gas?',\n",
       " 'output': 'The Lower explosion limit of Hydrogen gas is 4.0 vol%.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Identify the Lower explosion limit of Hydrogen gas?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
